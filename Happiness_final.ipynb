{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS1_final.ipynb","provenance":[{"file_id":"1vtmxiJqApvH0XqcT5nExxokHkEHmBVim","timestamp":1599759360063}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"8In3VbSuC-Tv","colab":{},"executionInfo":{"status":"ok","timestamp":1600078450893,"user_tz":-330,"elapsed":2834,"user":{"displayName":"Sireesha Jasti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYgAgXDUsF6Y4SoHIN99pKBDRQyEG7U0_oEL7Z8ps=s64","userId":"09863588337750692506"}}},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.pipeline import Pipeline\n","from matplotlib import pyplot\n","import operator\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.model_selection import GridSearchCV\n","import math\n","from sklearn.metrics import roc_auc_score\n","\n","%matplotlib inline\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import re\n","import sys\n","import pickle\n","from tqdm import tqdm\n","import os"],"execution_count":160,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-THhJnxJK_Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600078450896,"user_tz":-330,"elapsed":2811,"user":{"displayName":"Sireesha Jasti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYgAgXDUsF6Y4SoHIN99pKBDRQyEG7U0_oEL7Z8ps=s64","userId":"09863588337750692506"}},"outputId":"40abe53e-cbc4-4d40-aa0b-50eb9164ffa2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":161,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WACz4ztd9poH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1600078450897,"user_tz":-330,"elapsed":2795,"user":{"displayName":"Sireesha Jasti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYgAgXDUsF6Y4SoHIN99pKBDRQyEG7U0_oEL7Z8ps=s64","userId":"09863588337750692506"}},"outputId":"86b09dc3-b0a7-4fc2-eeb4-22063c1b1457"},"source":["df1 = pd.read_csv('/content/drive/My Drive/train_cs.csv')\n","print(df1.columns)"],"execution_count":162,"outputs":[{"output_type":"stream","text":["Index(['UserID', 'YOB', 'Gender', 'Income', 'HouseholdStatus',\n","       'EducationLevel', 'Party', 'Happy', 'Q124742', 'Q124122',\n","       ...\n","       'Q99716', 'Q99581', 'Q99480', 'Q98869', 'Q98578', 'Q98059', 'Q98078',\n","       'Q98197', 'Q96024', 'votes'],\n","      dtype='object', length=110)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tXkAHVsPrQ2d","colab_type":"text"},"source":["**As per the instructions in the class room instead of taking single observation, Here I am passing entore dataset**"]},{"cell_type":"code","metadata":{"id":"6t1bqkqUaQp3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600078450898,"user_tz":-330,"elapsed":2781,"user":{"displayName":"Sireesha Jasti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYgAgXDUsF6Y4SoHIN99pKBDRQyEG7U0_oEL7Z8ps=s64","userId":"09863588337750692506"}}},"source":["def happiness_prediction(df1):\n","\n","  #data preprocessing\n","\n","  #dropping UserID as it is an unimportant feature\n","  df1.drop(['UserID'], axis = 1, inplace = True)\n","  #features with missing values between 5-20% missing (mean/median is to be used to fill NAN values)\n","  features_mean = []\n","  for i in (df1.columns):\n","    if(df1[i].isnull().sum()*100/len(df1)) >=5 and (df1[i].isnull().sum()*100/len(df1))<=20:\n","      features_mean.append(i)\n","  \n","  df1['YOB'] = 2014 - df1['YOB']\n","\n","  df1 = df1.replace({'HouseholdStatus': {'Married (w/kids)' :3, 'Single (no kids)':0, 'Married (no kids)':2,'Domestic Partners (no kids)':4,'Domestic Partners (w/kids)':5, 'Single (w/kids)':1}})\n","  df1 = df1.replace({'EducationLevel': {'Master\\'s Degree':3, 'High School Diploma':0, 'Current K-12':1,'Current Undergraduate':2,'Bachelor\\'s Degree':2, 'Associate\\'s Degree':3,'Doctoral Degree':4}})\n","  df1['Income'] = df1.Income.map({'$25,001 - $50,000':1,'over $150,000':5,'$75,000 - $100,000':3,'$50,000 - $74,999':2,'under $25,000':0,'$100,001 - $150,000':4})\n","  df1['Party'] = df1.Party.map({'Independent':0,'Democrat':1,'Republican':2,'Libertarian':3,'Other':4})\n","  df1['Gender'] = df1.Gender.map({'Male':0, 'Female':1})\n","\n","  filter_col = [col for col in df1 if col.startswith('Q')]\n","  for col in filter_col: \n","    # converted yes/no or other feature string values to binary 0 or 1\n","    #print(col) from the given pdf file possible answers for the questions have been collected\n","    d = {'No': 0, 'Yes': 1, 'Only-child':0, 'Check!':0,'Nope':1,'Optimist':1,'Pessimist':0,'Mom':0,'Dad':1,'Rent':0,'Own':1, 'Yay people!':0,'Grrr people':1,'Online':0,'In-person':1,'Yes!':1,'Umm...':0,'Socialize':1,'Space':0,'Cautious':1,'Risk-friendly':0,'Mac':1,'PC':0,'Supportive':1,'Demanding':0,'Tunes':0,'Talk':1,'People':0,'Technology':1,'TMI':0,'Mysterious':1,'Start':0,'End':1,'Circumstances':0,'Me':1,'A.M.':0,'P.M.':1,'Happy':0,'Right':1,'Hot headed':0,'Cool headed':1,'Standard hours':0,'Odd hours':1,'Idealist':0,'Pragmatist':1,'Giving':0,'Receiving':1,'Study first':0,'Try first':1,'Science':0,'Art':1,'Public':0,'Private':1}\n","    df1[col] = df1[col].map(d).fillna(df1[col])\n","  \n","  Q1 = df1.quantile(0.25)\n","  Q3 = df1.quantile(0.75)\n","  IQR = Q3 - Q1\n","\n","  df1_TF = (df1 < (Q1 - 1.5 * IQR)) |(df1 > (Q3 + 1.5 * IQR))\n","\n","  for i in (df1.columns):# to get rid of outliers\n","    median = df1[i].quantile(0.50) # each feature median is taken\n","    outlier = df1[i].quantile(0.95) #  95th % value\n","    outlier_10 = df1[i].quantile(0.10)# 10th % value\n","    df1[i] = np.where(df1[i] > outlier , outlier, df1[i])# replace value above 95% with median\n","    df1[i] = np.where(df1[i] < outlier_10, outlier_10,df1[i])# replace value below 10% with median\n","\n","  #sklearn SimpleImputer is used to fill the missing values. The featuers 'Gender', \n","  #'HouseholdStatus', 'Party' are categorical.so instead of mean/median considered \n","  #'most_frequent' strategy from SimpleImputer. As YOB' is a numerical feature so it is filled with mean value\n","  df_mean = df1[features_mean]# features with missing values between 5-20% missing\n","  df_mean['YOB'].fillna(int(df_mean['YOB'].mean()), inplace=True)\n","  imp = SimpleImputer(strategy=\"most_frequent\")\n","  df_mean = imp.fit_transform(df1[features_mean])\n","\n","  for i in range(1,len(features_mean)):\n","    x = features_mean[i]\n","    df1[x] = df_mean[:,i].copy()\n","  \n","  df1['YOB'] = df_mean[:,0].copy()\n","  # taken columns with null values into 'cols_nan'\n","  cols_nan = df1.columns[df1.isna().any()].tolist()    \n","  # taken columns with non null values into 'cols_no_nan'\n","  cols_no_nan = df1.columns.difference(cols_nan).values    \n","  for col in cols_nan:\n","    test_data = df1[df1[col].isna()]# rows with null values are taken into testset\n","    train_data = df1.dropna()#remaining into train\n","    #Kneighbors classifier is applied\n","    knr = KNeighborsClassifier(n_neighbors=5).fit(train_data[cols_no_nan], train_data[col])\n","    #predict each test row with the trained model\n","    df1.loc[df1[col].isna(), col] = knr.predict(test_data[cols_no_nan])\n","    #updating columns with nan and without nan for updating train and test with new predicted rows\n","    cols_nan = df1.columns[df1.isna().any()].tolist()    \n","    cols_no_nan = df1.columns.difference(cols_nan).values\n","\n","  df_impu = df1.copy()\n","\n","  y = df_impu.Happy # separating the class label\n","  X = df_impu.drop(['Happy'],axis = 1) # taking remaining features\n","\n","  model = pickle.load(open(\"/content/drive/My Drive/final_model.pkl\",'rb'))\n","  predictions = model.predict(X)\n","  score = model.score(X,y)\n","  print(score)\n","  \n","  return(predictions)"],"execution_count":163,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNzBjf4NjpSr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1600078462097,"user_tz":-330,"elapsed":13966,"user":{"displayName":"Sireesha Jasti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYgAgXDUsF6Y4SoHIN99pKBDRQyEG7U0_oEL7Z8ps=s64","userId":"09863588337750692506"}},"outputId":"0e6cf8a2-86e7-4807-c632-32aee0f5174b"},"source":["import time\n","\n","start_time = time.clock()\n","predictions = happiness_prediction(df1)\n","print (time.clock() - start_time, \"seconds\")\n","\n","print(predictions)"],"execution_count":164,"outputs":[{"output_type":"stream","text":["0.7018835245724183\n","10.795162999999974 seconds\n","[1. 1. 1. ... 0. 0. 1.]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0SDh-37KhDUS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1600078462098,"user_tz":-330,"elapsed":13949,"user":{"displayName":"Sireesha Jasti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYgAgXDUsF6Y4SoHIN99pKBDRQyEG7U0_oEL7Z8ps=s64","userId":"09863588337750692506"}},"outputId":"3650c6ed-6630-42a9-ee5c-04fb434ab8f3"},"source":["%%time\n","def error_predict(y,predictions):\n","  Accuracy = roc_auc_score(df1['Happy'],predictions)\n","  return (Accuracy)"],"execution_count":165,"outputs":[{"output_type":"stream","text":["CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n","Wall time: 11.7 µs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TADhkGrlmeGs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600078462100,"user_tz":-330,"elapsed":13935,"user":{"displayName":"Sireesha Jasti","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYgAgXDUsF6Y4SoHIN99pKBDRQyEG7U0_oEL7Z8ps=s64","userId":"09863588337750692506"}},"outputId":"1b622144-b629-4db9-d4ab-ceed2b780b5f"},"source":["Accuracy = error_predict(y,predictions)\n","print(Accuracy)"],"execution_count":166,"outputs":[{"output_type":"stream","text":["0.688003663003663\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XOnss6Adf79M","colab_type":"text"},"source":["1) Business Problem:  income, education, social support, freedom” are the main criteria to measure happiness . Our dataset is containing all these  features, along with informal questions  which are predicting the happiness of the user and extracting  insights of the data.\n","\n","One such example is analyzing the factors of a job, which are the largest predictors of workplace satisfaction. Here we can predict overall satisfaction of an employee in the job, based on various attribures like compensation, benifits, flexibility, oppurtunity for advancement etc.\n","\n","2)ROC AUC answers the question of how well the model discriminates between the two classes. It doesn't tell you anything about the costs of different kinds of errors. \n","\n","3) Precision is the percentage of your results which are relevant. recall refers to the percentage of total relevant results correctly classified by your algorithm.\n","\n","For rare cancer data modeling (whre most of the class labels are nehgative means only few are having cancer), this is an imbalanced data. In this case anything that doesn't account for false-negatives is a crime. In such cases 'Recall' is a better measure than 'precision'.\n","\n","For YouTube recommendations,happiness prediction false-negatives is less of a concern. In such cases 'Precision' is better here.\n","\n","so in our case 'Precision' is more important than 'Recall'\n"]}]}